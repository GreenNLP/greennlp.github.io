---
title: Efficient training
sections:
  - Throughput per compute cycle
  - Mixed precision training
  - Efficient power usage and the relation to utilization or GPU core voltage
---

Following text contains a lot of placeholders and is still work in process.

### Throughput per compute cycle

Interconnects? Software?

### Mixed precision training

TODO

### Efficient power usage and the relation to utilization or GPU core voltage

GPU voltage/frequency curves tend to flatten out the higher you go. In terms of power per compute, the efficiency tends to drop when going for the highest clock speed and utilization. TODO

[Graph here]

